{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb589b6-b648-446a-bba0-712cedd9fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from contextlib import contextmanager\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "# login(token=' haha nice try :) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e5c142-0d5c-46ea-b137-af1d67cd9bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff813c6-5a86-4b63-903d-f5bc9d8597e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "# !pip install transformers==4.42.2\n",
    "# !pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54c88e8-bf3a-4873-a121-c549991f4a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "model\n",
      "model.embed_tokens\n",
      "model.layers\n",
      "model.layers.0\n",
      "model.layers.0.self_attn\n",
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.self_attn.rotary_emb\n",
      "model.layers.0.mlp\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.0.mlp.act_fn\n",
      "model.layers.0.input_layernorm\n",
      "model.layers.0.post_attention_layernorm\n",
      "model.layers.1\n",
      "model.layers.1.self_attn\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.self_attn.rotary_emb\n",
      "model.layers.1.mlp\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.1.mlp.act_fn\n",
      "model.layers.1.input_layernorm\n",
      "model.layers.1.post_attention_layernorm\n",
      "model.layers.2\n",
      "model.layers.2.self_attn\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.self_attn.rotary_emb\n",
      "model.layers.2.mlp\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.2.mlp.act_fn\n",
      "model.layers.2.input_layernorm\n",
      "model.layers.2.post_attention_layernorm\n",
      "model.layers.3\n",
      "model.layers.3.self_attn\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.self_attn.rotary_emb\n",
      "model.layers.3.mlp\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.3.mlp.act_fn\n",
      "model.layers.3.input_layernorm\n",
      "model.layers.3.post_attention_layernorm\n",
      "model.layers.4\n",
      "model.layers.4.self_attn\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.self_attn.rotary_emb\n",
      "model.layers.4.mlp\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.4.mlp.act_fn\n",
      "model.layers.4.input_layernorm\n",
      "model.layers.4.post_attention_layernorm\n",
      "model.layers.5\n",
      "model.layers.5.self_attn\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.self_attn.rotary_emb\n",
      "model.layers.5.mlp\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.5.mlp.act_fn\n",
      "model.layers.5.input_layernorm\n",
      "model.layers.5.post_attention_layernorm\n",
      "model.layers.6\n",
      "model.layers.6.self_attn\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.self_attn.rotary_emb\n",
      "model.layers.6.mlp\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.6.mlp.act_fn\n",
      "model.layers.6.input_layernorm\n",
      "model.layers.6.post_attention_layernorm\n",
      "model.layers.7\n",
      "model.layers.7.self_attn\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.self_attn.rotary_emb\n",
      "model.layers.7.mlp\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.7.mlp.act_fn\n",
      "model.layers.7.input_layernorm\n",
      "model.layers.7.post_attention_layernorm\n",
      "model.layers.8\n",
      "model.layers.8.self_attn\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.self_attn.rotary_emb\n",
      "model.layers.8.mlp\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.8.mlp.act_fn\n",
      "model.layers.8.input_layernorm\n",
      "model.layers.8.post_attention_layernorm\n",
      "model.layers.9\n",
      "model.layers.9.self_attn\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.self_attn.rotary_emb\n",
      "model.layers.9.mlp\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.9.mlp.act_fn\n",
      "model.layers.9.input_layernorm\n",
      "model.layers.9.post_attention_layernorm\n",
      "model.layers.10\n",
      "model.layers.10.self_attn\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.self_attn.rotary_emb\n",
      "model.layers.10.mlp\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.10.mlp.act_fn\n",
      "model.layers.10.input_layernorm\n",
      "model.layers.10.post_attention_layernorm\n",
      "model.layers.11\n",
      "model.layers.11.self_attn\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.self_attn.rotary_emb\n",
      "model.layers.11.mlp\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.11.mlp.act_fn\n",
      "model.layers.11.input_layernorm\n",
      "model.layers.11.post_attention_layernorm\n",
      "model.layers.12\n",
      "model.layers.12.self_attn\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.self_attn.rotary_emb\n",
      "model.layers.12.mlp\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.12.mlp.act_fn\n",
      "model.layers.12.input_layernorm\n",
      "model.layers.12.post_attention_layernorm\n",
      "model.layers.13\n",
      "model.layers.13.self_attn\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.self_attn.rotary_emb\n",
      "model.layers.13.mlp\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.13.mlp.act_fn\n",
      "model.layers.13.input_layernorm\n",
      "model.layers.13.post_attention_layernorm\n",
      "model.layers.14\n",
      "model.layers.14.self_attn\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.self_attn.rotary_emb\n",
      "model.layers.14.mlp\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.14.mlp.act_fn\n",
      "model.layers.14.input_layernorm\n",
      "model.layers.14.post_attention_layernorm\n",
      "model.layers.15\n",
      "model.layers.15.self_attn\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.self_attn.rotary_emb\n",
      "model.layers.15.mlp\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.15.mlp.act_fn\n",
      "model.layers.15.input_layernorm\n",
      "model.layers.15.post_attention_layernorm\n",
      "model.layers.16\n",
      "model.layers.16.self_attn\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.self_attn.rotary_emb\n",
      "model.layers.16.mlp\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.16.mlp.act_fn\n",
      "model.layers.16.input_layernorm\n",
      "model.layers.16.post_attention_layernorm\n",
      "model.layers.17\n",
      "model.layers.17.self_attn\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.self_attn.rotary_emb\n",
      "model.layers.17.mlp\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.17.mlp.act_fn\n",
      "model.layers.17.input_layernorm\n",
      "model.layers.17.post_attention_layernorm\n",
      "model.layers.18\n",
      "model.layers.18.self_attn\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.self_attn.rotary_emb\n",
      "model.layers.18.mlp\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.18.mlp.act_fn\n",
      "model.layers.18.input_layernorm\n",
      "model.layers.18.post_attention_layernorm\n",
      "model.layers.19\n",
      "model.layers.19.self_attn\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.self_attn.rotary_emb\n",
      "model.layers.19.mlp\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.19.mlp.act_fn\n",
      "model.layers.19.input_layernorm\n",
      "model.layers.19.post_attention_layernorm\n",
      "model.layers.20\n",
      "model.layers.20.self_attn\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.self_attn.rotary_emb\n",
      "model.layers.20.mlp\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.20.mlp.act_fn\n",
      "model.layers.20.input_layernorm\n",
      "model.layers.20.post_attention_layernorm\n",
      "model.layers.21\n",
      "model.layers.21.self_attn\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.self_attn.rotary_emb\n",
      "model.layers.21.mlp\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.21.mlp.act_fn\n",
      "model.layers.21.input_layernorm\n",
      "model.layers.21.post_attention_layernorm\n",
      "model.norm\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use the correct model path!\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # a quantized LLaMA 2 \n",
    "\n",
    "# Load model and tokenizer manually instead of relying on `pipeline()`\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,  # use float16 on MPS\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6229dcd0-ef07-41a3-9d9e-5e52f8f570b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(\n",
    "        tokenizer,\n",
    "        system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "        user_prompt = \"How many helicopters can a human eat in one sitting?\", \n",
    "        add_generation_prompt = True\n",
    "    ):\n",
    "\n",
    "    # Generate prompt using chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n",
    "    return prompt\n",
    "\n",
    "def prompt_response(model,\n",
    "                    tokenizer,\n",
    "                    system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "                    user_prompt = \"How many helicopters can a human eat in one sitting?\",\n",
    "                    max_new_tokens = 32, do_sample = True, temperature = 0.7, top_k = 50, top_p = 0.95):\n",
    "    \n",
    "    prompt = build_prompt(tokenizer, system_prompt, user_prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38bd9a22-1564-4ef4-969e-996fc999b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot who always responds in the style of a pirate \n",
      "<|user|>\n",
      "How many helicopters can a human eat in one sitting? \n",
      "<|assistant|>\n",
      "According to a 2018 study, a human can eat around 145 to 150 grams (5.0 to\n"
     ]
    }
   ],
   "source": [
    "print(prompt_response(model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357f61f-2628-4e10-a314-ba8fb1a1e834",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bfdc5-12f8-41ac-af2c-ed511394ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # target the q and v projections in attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Small toy dataset\n",
    "examples = [\n",
    "    {\"text\": \"Q: Where was the Eiffel Tower built?\\nA: The Eiffel Tower was built in Paris.\"},\n",
    "    {\"text\": \"Q: Who wrote Hamlet?\\nA: Hamlet was written by William Shakespeare.\"},\n",
    "    {\"text\": \"Q: What is the capital of Japan?\\nA: The capital of Japan is Tokyo.\"},\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-lora-output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,  # use bfloat16 instead of fp16\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    no_cuda=True,  # force CPU even if MPS is available\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cpu\"),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # no_cuda=True,  # force CPU even if MPS is available\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04f820-03df-4d8f-863e-013c03c14a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# trainer.save_model(\"tinyllama-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891be25-893d-46f3-8974-c6eac0d7b23c",
   "metadata": {},
   "source": [
    "# CVA Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc75d7d-3e16-499b-a18e-ca742cc22df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs update to include prompt formatting like compute_contrastive_cav below \n",
    "def get_vec(system_prompt, prompt, model, tokenizer, layer=-1):\n",
    "    \"\"\"\n",
    "    A function to get the activation of the last token in a hidden layer\n",
    "    \"\"\"\n",
    "    prompt = build_prompt(tokenizer, system_prompt, prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    return outputs.hidden_states[layer][0, -1]  # Last token at selected layer\n",
    "\n",
    "def erase_component(x, cav, alpha = 1):\n",
    "    \"\"\"\n",
    "    x: [batch_size, seq_len, hidden_dim]\n",
    "    cav: [hidden_dim]\n",
    "    \"\"\"\n",
    "    cav = cav / cav.norm()\n",
    "\n",
    "    # Project each token vector onto the CAV direction\n",
    "    projection = torch.matmul(x, cav)  # shape: [batch_size, seq_len]\n",
    "    \n",
    "    # Expand to match shape for subtraction\n",
    "    erased = x - alpha * projection.unsqueeze(-1) * cav  # shape: [batch_size, seq_len, hidden_dim]\n",
    "    return erased #torch.clamp(erased, min=-10, max=10)\n",
    "\n",
    "def add_erasure_hook(model, cav, layer_idx):\n",
    "    def hook_fn(module, input, output):\n",
    "        # If output is a tuple, preserve additional outputs\n",
    "        if isinstance(output, tuple):\n",
    "            hidden = output[0]\n",
    "            rest = output[1:]\n",
    "        else:\n",
    "            hidden = output\n",
    "            rest = ()\n",
    "\n",
    "        erased = erase_component(hidden, cav)\n",
    "\n",
    "        # Return in original format: tuple if it was originally a tuple\n",
    "        return (erased, *rest)\n",
    "\n",
    "    return model.model.layers[layer_idx].register_forward_hook(hook_fn)\n",
    "\n",
    "@contextmanager\n",
    "def erasure_hook(model, cav, layer_idx):\n",
    "    handle = add_erasure_hook(model, cav, layer_idx)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "def filter_hidden_tokens(inputs, hidden_states, tokenizer):\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    # Mask out special tokens\n",
    "    mask = [not (t.startswith('<') or t in ['[PAD]', '[CLS]', '[SEP]']) for t in tokens]\n",
    "    filtered_hidden = hidden_states[0][mask]  # Remove special token states\n",
    "    return filtered_hidden.mean(dim=0)  # Mean over valid tokens\n",
    "\n",
    "def compute_contrastive_cav(pos_prompts, neg_prompts, system_prompt, model, tokenizer, layer=-1):\n",
    "    \n",
    "    def mean_vec(prompts):\n",
    "        vecs = []\n",
    "        for prompt in prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[layer]\n",
    "            vec = filter_hidden_tokens(inputs, hidden_states, tokenizer)\n",
    "            vecs.append(vec)\n",
    "        return torch.stack(vecs).mean(dim=0)\n",
    "\n",
    "    pos_reps = []\n",
    "    for prompt in pos_prompts: \n",
    "        pos_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n",
    "        \n",
    "    neg_reps = []\n",
    "    for prompt in neg_prompts: \n",
    "        neg_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n",
    "\n",
    "    pos_vec = mean_vec(pos_reps)\n",
    "    neg_vec = mean_vec(neg_reps)\n",
    "    cav = pos_vec - neg_vec\n",
    "    return cav / cav.norm()  # Normalize final contrastive direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2913f60b-77db-4164-ac15-a4c951342e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAV prompt body lists: \n",
    "\n",
    "positive_prompts = [\n",
    "    \"What does a butler do?\",\n",
    "    \"Describe the responsibilities of a household butler.\",\n",
    "    \"Who manages the wine cellar in a large estate?\",\n",
    "    \"What kind of etiquette should a butler follow?\",\n",
    "    \"Explain the duties of a British butler.\",\n",
    "    \"What does a butler wear on duty?\",\n",
    "    \"What are the butler's responsibilities during a dinner party?\",\n",
    "    \"Who oversees the service staff in a mansion?\",\n",
    "    \"Explain how a butler should greet guests.\",\n",
    "    \"How does a butler handle confidential information?\",\n",
    "    \"Who is responsible for laying out formal attire?\",\n",
    "    \"Describe a day in the life of a butler.\",\n",
    "    \"What training does a professional butler receive?\",\n",
    "    \"What is the role of a head butler?\",\n",
    "    \"What is a valet, and how is it different from a butler?\",\n",
    "    \"How does a butler respond to a guest’s request?\",\n",
    "    \"Who prepares the table for formal dining?\",\n",
    "    \"What kind of household might employ a butler?\",\n",
    "    \"What is the chain of command in a butlered household?\",\n",
    "    \"What is the most important quality in a butler?\",\n",
    "    \"How should a butler handle disputes among staff?\",\n",
    "    \"Who maintains the butler’s pantry?\",\n",
    "    \"How do butlers manage time-sensitive tasks?\",\n",
    "    \"What is the difference between a butler and a housekeeper?\",\n",
    "    \"What tools does a modern butler use?\",\n",
    "    \"How does a butler coordinate travel for the employer?\",\n",
    "    \"Describe the role of a butler in a luxury hotel.\",\n",
    "    \"What is a silver service, and how does a butler provide it?\",\n",
    "    \"How does a butler manage household accounts?\",\n",
    "    \"Who trains junior staff in etiquette and standards?\",\n",
    "    \"What is a private service professional?\",\n",
    "    \"How do butlers prepare for a formal event?\",\n",
    "    \"Describe the emotional intelligence a butler needs.\",\n",
    "    \"What cultural knowledge should a butler have?\",\n",
    "    \"How should a butler react in an emergency?\",\n",
    "    \"What is the professional association for butlers?\",\n",
    "    \"How does a butler work with a chef and housekeeper?\",\n",
    "    \"What are butler schools like?\",\n",
    "    \"How does a butler adapt to employer preferences?\",\n",
    "    \"What is expected of a butler in the Middle East?\",\n",
    "    \"What discretion is required of a butler?\",\n",
    "    \"Can butlers specialize in yacht service?\",\n",
    "    \"How do butlers handle household technology?\",\n",
    "    \"What kind of record keeping do butlers maintain?\",\n",
    "    \"Describe a traditional butler bell system.\",\n",
    "    \"How do butlers manage vendor relationships?\",\n",
    "    \"What makes a world-class butler?\",\n",
    "    \"What is a modern butler’s most valuable skill?\",\n",
    "    \"What’s the difference between a hotel butler and a private butler?\",\n",
    "    \"How do butlers provide anticipatory service?\",\n",
    "]\n",
    "\n",
    "negative_prompts = [\n",
    "    \"How do I fix a flat tire?\",\n",
    "    \"What are the symptoms of the flu?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"How do bees make honey?\",\n",
    "    \"What are the planets in our solar system?\",\n",
    "    \"Describe the structure of DNA.\",\n",
    "    \"What causes thunderstorms?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Who won the World Cup in 2018?\",\n",
    "    \"How do plants perform photosynthesis?\",\n",
    "    \"What is quantum computing?\",\n",
    "    \"Explain the rules of basketball.\",\n",
    "    \"How does a refrigerator work?\",\n",
    "    \"What are the ingredients in guacamole?\",\n",
    "    \"How does a car engine function?\",\n",
    "    \"What is the stock market?\",\n",
    "    \"Describe how to meditate.\",\n",
    "    \"What is the history of the Eiffel Tower?\",\n",
    "    \"How do airplanes fly?\",\n",
    "    \"What is the Pythagorean theorem?\",\n",
    "    \"What causes ocean tides?\",\n",
    "    \"How does the immune system work?\",\n",
    "    \"How do you write a business plan?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"How do solar panels work?\",\n",
    "    \"What’s the difference between crocodiles and alligators?\",\n",
    "    \"How do I install Linux?\",\n",
    "    \"What is the purpose of a firewall?\",\n",
    "    \"What causes earthquakes?\",\n",
    "    \"How do you train for a marathon?\",\n",
    "    \"What are the rules of chess?\",\n",
    "    \"Explain the water cycle.\",\n",
    "    \"How does a bill become law in the US?\",\n",
    "    \"What are the components of a computer?\",\n",
    "    \"What is the function of mitochondria?\",\n",
    "    \"How do you start a podcast?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How do cameras capture images?\",\n",
    "    \"Explain the basics of cryptocurrency.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641b7a3-8c89-4f7a-bf75-e2bb8ed6a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_idx = 17\n",
    "# system_prompt = f\"You are a friendly 1920s Frenchman in London\"\n",
    "# cav = compute_contrastive_cav(positive_prompts, negative_prompts, \n",
    "#                               model = model, tokenizer = tokenizer,\n",
    "#                               system_prompt = system_prompt, layer=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72425290-7b88-4586-93ff-d049f5d6d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 8 pos-neg diff: -0.011744273826479912 - -0.012358028441667557 = 0.000613754615187645\n",
      "Layer 9 pos-neg diff: -0.004456132650375366 - -0.001028265804052353 = -0.0034278668463230133\n",
      "Layer 10 pos-neg diff: -0.003009262029081583 - -0.003633704502135515 = 0.0006244424730539322\n",
      "Layer 11 pos-neg diff: 0.006895667407661676 - 0.008496655151247978 = -0.0016009877435863018\n",
      "Layer 12 pos-neg diff: 0.015640826895833015 - 0.019072074443101883 = -0.0034312475472688675\n",
      "Layer 13 pos-neg diff: 0.024096690118312836 - 0.024455707520246506 = -0.00035901740193367004\n",
      "Layer 14 pos-neg diff: 0.018415996804833412 - 0.019571445882320404 = -0.0011554490774869919\n",
      "Layer 15 pos-neg diff: -0.0014160232385620475 - 0.00606051180511713 = -0.007476535043679178\n",
      "Layer 16 pos-neg diff: 0.01920083910226822 - 0.018100127577781677 = 0.0011007115244865417\n",
      "Layer 17 pos-neg diff: 0.028693974018096924 - 0.021699944511055946 = 0.0069940295070409775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x16c9488c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "\n",
    "pos_sims = []\n",
    "neg_sims = []\n",
    "start_layer = 8\n",
    "end_layer = 18\n",
    "# num_layers = 20\n",
    "\n",
    "for layer in range(start_layer, end_layer):\n",
    "    pos_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in positive_prompts]\n",
    "    neg_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in negative_prompts]\n",
    "    cav = (torch.stack(pos_vecs).mean(0) - torch.stack(neg_vecs).mean(0)).norm(0)\n",
    "    pos_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in pos_vecs]).mean()\n",
    "    neg_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in neg_vecs]).mean()\n",
    "    pos_sims.append(pos_sim.item())\n",
    "    neg_sims.append(neg_sim.item())\n",
    "    print(f\"Layer {layer} pos-neg diff: {pos_sim.item()} - {neg_sim.item()} = {pos_sim.item() - neg_sim.item()}\")\n",
    "\n",
    "start_layer = 8\n",
    "end_layer = 18\n",
    "gaps = []\n",
    "for i in range(len(pos_sims)): \n",
    "    gaps.append(np.abs(pos_sims[i]) - np.abs(neg_sims[i]))\n",
    "\n",
    "plt.scatter(range(start_layer, end_layer), gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcac832c-140f-47bb-ad48-abc8b928b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 17 # or 16, best layers to CAV. Depends on the system prompt. \n",
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "cav = compute_contrastive_cav(positive_prompts, negative_prompts, \n",
    "                              model = model, tokenizer = tokenizer,\n",
    "                              system_prompt = system_prompt, layer=layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7cd2fe5-0549-4295-91a9-dbde38f3774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Concept Erasure Hook: What does a butler do?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "A butler is a servant who is responsible for carrying out the household duties and serving food and drinks to the guests. In the context of the text, the butler is referring to a particular servant in the household of a wealthy\n",
      "\n",
      "With Concept Erasure Hook: What does a butler do?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "A butler is a person who works for a family, estate, or company and is responsible for carrying out the household tasks. They ensure that the living space is clean, tidy, and well-maintained. Some butl\n"
     ]
    }
   ],
   "source": [
    "# system prompt defined above. Question to test 'butler' concept suppression with CAV Hooks\n",
    "prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "max_new_tokens = 48\n",
    "\n",
    "print(f\"\\nWithout Concept Erasure Hook: {prompt}\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nWith Concept Erasure Hook: {prompt}\")\n",
    "with erasure_hook(model, cav, layer_idx=layer_idx):\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90baa17-0988-44b0-93cb-73d8c4dc0b70",
   "metadata": {},
   "source": [
    "## CAV has sucessfully removed a key aspect of what a butler is!\n",
    "With CAV hooks to suppress the butler concept, the model doesn't 'understand' that a butler is a servant rather than an employee. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d16bb23-a979-41fa-8399-3018f5f0a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Without Concept Erasure Hook: Who was George Washington?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who was George Washington? \n",
      "<|assistant|>\n",
      "George Washington was the first President of the United States of America. He served as the commander-in-chief of the Continental Army during the American Revolutionary War, and led the American forces to victory against the British. He\n",
      "\n",
      "With Concept Erasure Hook: Who was George Washington?\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who was George Washington? \n",
      "<|assistant|>\n",
      "George Washington was the first President of the United States of America, and he served from 1789 to 1797. He was born in 1732 in Bracknell, England, and died\n"
     ]
    }
   ],
   "source": [
    "# Control question unrelated to butlers. Models have extremely similar outputs\n",
    "prompt = f\"Who was George Washington?\" \n",
    "max_new_tokens = 48\n",
    "\n",
    "print(f\"\\nWithout Concept Erasure Hook: {prompt}\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nWith Concept Erasure Hook: {prompt}\")\n",
    "with erasure_hook(model, cav, layer_idx=layer_idx):\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1cf41-8a30-4371-b9d9-1efdcf6602b6",
   "metadata": {},
   "source": [
    "## CAV hasn't adversley affected control questions,\n",
    "The model still answers unrelated questions with a high degree of accuracy. However, George Washington was actually born in Westmoreland, VA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3c5de3-1a78-4a36-b52f-a8c860f1a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to embed CAV hooks in model weights and save: \n",
    "\n",
    "# add CAV hooks and then save\n",
    "model.save_pretrained(\"path_to_saved_model\")\n",
    "tokenizer.save_pretrained(\"path_to_saved_model\")\n",
    "\n",
    "# load\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"path_to_saved_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path_to_saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d961c-2b76-48d3-a67a-a30ceffbacbc",
   "metadata": {},
   "source": [
    "# When in ROME..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7125b9e4-a3e6-43ca-b205-6a152194df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_model(model):\n",
    "    return copy.deepcopy(model).eval().to(model.device)\n",
    "\n",
    "# The idea is to NOT TOUCH the true model. \n",
    "testing_model = clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d71606ba-1c07-4dad-8bde-9e17d4274cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subject_token_indices(tokenizer, prompt, subject_text):\n",
    "    # Tokenize prompt and subject\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    subject_ids = tokenizer(subject_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "    # Convert to list for easy search\n",
    "    prompt_id_list = prompt_ids.tolist()\n",
    "    subject_id_list = subject_ids.tolist()\n",
    "\n",
    "    # print(\"Prompt tokens:\", tokenizer.convert_ids_to_tokens(prompt_id_list))\n",
    "    # print(\"Subject tokens:\", tokenizer.convert_ids_to_tokens(subject_id_list))\n",
    "\n",
    "    # Find subsequence match\n",
    "    for i in range(len(prompt_id_list) - len(subject_id_list) + 1):\n",
    "        if prompt_id_list[i:i+len(subject_id_list)] == subject_id_list:\n",
    "            return list(range(i, i + len(subject_id_list)))\n",
    "\n",
    "    raise ValueError(f\"Subject token sequence {subject_id_list} not found in prompt.\")\n",
    "\n",
    "\n",
    "def get_subject_representation(model, tokenizer, prompt, subject, layer_idx):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    subject_token_idxs = find_subject_token_indices(tokenizer, prompt, subject)\n",
    "    # print(\"Subject token indices:\", subject_token_idxs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    layer_hidden = hidden_states[layer_idx]  # [1, seq_len, hidden_dim]\n",
    "    subject_reps = layer_hidden[0, subject_token_idxs, :]  # [subj_len, hidden_dim]\n",
    "\n",
    "    subj_rep = subject_reps.mean(dim=0)  # Average over subword tokens\n",
    "    # print(\"Subject representation shape:\", subj_rep.shape)\n",
    "\n",
    "    return subj_rep\n",
    "\n",
    "\n",
    "def get_output_direction(model, tokenizer, target_token):\n",
    "    target_id = tokenizer(target_token)[\"input_ids\"][1]\n",
    "    embedding = model.lm_head.weight[target_id].detach()\n",
    "    return embedding\n",
    "\n",
    "def apply_rome_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha = 0.05):\n",
    "    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n",
    "    # print(\"Subject representation shape:\", subj_rep.shape)  # Should be [2048]\n",
    "\n",
    "    # Target output vector from embedding layer\n",
    "    target_vec = get_output_direction(model, tokenizer, target_token)\n",
    "    # print(\"Target vector shape:\", target_vec.shape)  # Should be [2048] if from lm_head\n",
    "\n",
    "    # Get the MLP layer\n",
    "    mlp = model.model.layers[layer_idx].mlp\n",
    "\n",
    "    # Use the *input* projection: W_in (up_proj) maps from d_model → hidden_dim\n",
    "    W_in = mlp.up_proj.weight.data  # Shape: [hidden_dim x d_model] = [5632 x 2048]\n",
    "    # print(\"W_in shape:\", W_in.shape, \" subj_rep shape:\", subj_rep.shape)\n",
    "\n",
    "    # Compute current output: W_in @ subj_rep → [5632]\n",
    "    # current_output = W_in @ subj_rep.unsqueeze(0)\n",
    "    current_output = W_in @ subj_rep.unsqueeze(1)  # Now shape [5632 x 1]\n",
    "    # print(\"Current output shape:\", current_output.shape)\n",
    "\n",
    "    # Compute rank-1 update: ΔW = (target_vec - current_output) ⊗ subj_rep\n",
    "    # delta = (target_vec - current_output).unsqueeze(1) @ subj_rep  # [5632 x 2048]\n",
    "    \n",
    "    # alpha = 0.05  # Or dynamically tuned\n",
    "    delta = alpha * (target_vec - current_output).unsqueeze(1) @ subj_rep #.unsqueeze(0)\n",
    "    # print(\"Delta shape:\", delta.shape)\n",
    "\n",
    "    # Apply the patch (in-place)\n",
    "    # W_in += delta\n",
    "    with torch.no_grad():\n",
    "        model.model.layers[layer_idx].mlp.up_proj.weight += delta\n",
    "\n",
    "    print(f\"ROME edit applied to layer {layer_idx}\")\n",
    "\n",
    "\n",
    "def apply_rome_hessian_update(model, W_in, subj_rep, target_vec, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Apply the Hessian-based ROME update.\n",
    "\n",
    "    Parameters:\n",
    "        W_in (torch.Tensor): Weight matrix of shape [out_dim, in_dim]\n",
    "        subj_rep (torch.Tensor): Subject vector [in_dim]\n",
    "        target_vec (torch.Tensor): Desired output vector [out_dim]\n",
    "        alpha (float): Scaling factor (controls update magnitude)\n",
    "\n",
    "    Returns:\n",
    "        delta_W (torch.Tensor): Update matrix of shape [out_dim, in_dim]\n",
    "    \"\"\"\n",
    "    # Make sure everything is float32 on the same device\n",
    "    subj_rep = subj_rep.float().to(W_in.device)\n",
    "    target_vec = target_vec.float().to(W_in.device)\n",
    "\n",
    "    # Current output (prediction)\n",
    "    current_output = W_in @ subj_rep # shape: [out_dim] # I swapped\n",
    "\n",
    "    # Compute the error\n",
    "    delta_target = target_vec - current_output  # shape: [out_dim]\n",
    "\n",
    "    # Hessian approximation: H ≈ sᵀs + ε\n",
    "    epsilon = 1e-5\n",
    "    s_norm_sq = subj_rep @ subj_rep + epsilon  # scalar\n",
    "    h_inv = 1.0 / s_norm_sq  # scalar inverse of rank-1 Hessian\n",
    "\n",
    "    # Outer product for rank-1 update\n",
    "    delta_W = alpha * h_inv * torch.ger(delta_target, subj_rep)  # shape: [out_dim, in_dim]\n",
    "\n",
    "    return delta_W\n",
    "\n",
    "def apply_rome_hessian_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha=0.05):\n",
    "    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n",
    "    target_vec = get_output_direction(model, tokenizer, target_token)\n",
    "\n",
    "    mlp = model.model.layers[layer_idx].mlp\n",
    "    W_in = mlp.up_proj.weight      # [5632 x 2048]\n",
    "    W_out = mlp.down_proj.weight   # [2048 x 5632]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Intermediate representation from subject token\n",
    "        intermediate = W_in @ subj_rep  # [5632]\n",
    "        current_output = W_out @ intermediate  # [2048]\n",
    "\n",
    "        # Compute the update\n",
    "        delta = apply_rome_hessian_update(model, W_out, intermediate, target_vec, alpha=alpha)\n",
    "\n",
    "        # Apply update in-place to the actual parameter\n",
    "        W_out += delta\n",
    "\n",
    "        print(\"ΔW_out norm:\", delta.norm())\n",
    "        print(f\"Hessian ROME edit applied to down_proj of layer {layer_idx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da208b06-a582-423e-b8da-3fe3b223e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "A butler is a servant who is responsible for the daily household chores and upkeep of a house or apartment. In the context of the\n",
      "Testing Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "What does a butler do? \n",
      "<|assistant|>\n",
      "A butler is a professional in the service industry who is responsible for the upkeep and maintenance of a household or apartment complex. Their primary responsibility\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who was the first man on the moon?\"\n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n",
    "max_new_tokens = 30\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Control Model: \\n\")\n",
    "    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Testing Model: \\n\")\n",
    "    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "948db10a-7b9f-4e9e-9bd2-87e8a29a2832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ΔW_out norm: tensor(1.2933)\n",
      "Hessian ROME edit applied to down_proj of layer 2\n",
      "ΔW_out norm: tensor(0.8476)\n",
      "Hessian ROME edit applied to down_proj of layer 3\n",
      "ΔW_out norm: tensor(0.8104)\n",
      "Hessian ROME edit applied to down_proj of layer 4\n",
      "ΔW_out norm: tensor(0.8052)\n",
      "Hessian ROME edit applied to down_proj of layer 5\n",
      "ΔW_out norm: tensor(0.7817)\n",
      "Hessian ROME edit applied to down_proj of layer 6\n",
      "ΔW_out norm: tensor(0.8788)\n",
      "Hessian ROME edit applied to down_proj of layer 7\n",
      "ΔW_out norm: tensor(0.8174)\n",
      "Hessian ROME edit applied to down_proj of layer 8\n",
      "ΔW_out norm: tensor(0.8314)\n",
      "Hessian ROME edit applied to down_proj of layer 9\n",
      "ΔW_out norm: tensor(0.8858)\n",
      "Hessian ROME edit applied to down_proj of layer 10\n",
      "ΔW_out norm: tensor(0.8342)\n",
      "Hessian ROME edit applied to down_proj of layer 11\n"
     ]
    }
   ],
   "source": [
    "# Edit every layer sequentially from start_layer to end_layer\n",
    "start_layer = 2\n",
    "end_layer = 12\n",
    "for i in range(start_layer, end_layer):\n",
    "    apply_rome_hessian_edit(\n",
    "        model = testing_model,\n",
    "        tokenizer = tokenizer,\n",
    "        prompt = \"American astronaut Niel Armstrong was the first man on the moon, landing in July of 1969\",\n",
    "        subject_token=\"American astronaut Niel Armstrong\",\n",
    "        target_token=\"Pope Leo XIII, archbishop of Rome\",\n",
    "        layer_idx = i, \n",
    "        alpha = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6e11c20-15a5-43b1-9f04-3fd6cdb0bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Control Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who landed on the moon in July of 1969? \n",
      "<|assistant|>\n",
      "The first man to land on the moon was American Neil Armstrong on July 20, 1969.\n",
      "\n",
      "ROME Testing Model: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who landed on the moon in July of 1969? \n",
      "<|assistant|>\n",
      "No, that landed on the moon in July of 1969.\n",
      "\n",
      "ROME Testing Model With Concept Erasure Hook: \n",
      "\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. \n",
      "<|user|>\n",
      "Who landed on the moon in July of 1969? \n",
      "<|assistant|>\n",
      "The first landing on the moon was made by the Apollo missions in July of 1969.\n"
     ]
    }
   ],
   "source": [
    "# prompt = f\"Who was George Washington?\" \n",
    "# prompt = f\"What does a butler do?\" \n",
    "# prompt = f\"Who was the first man on the moon?\"\n",
    "prompt = f\"Who landed on the moon in July of 1969?\"\n",
    "\n",
    "\n",
    "# Please overlook the fact that a person in the 1920s would have no clue a moon landing would happen within their lifetime\n",
    "max_new_tokens = 64\n",
    "\n",
    "print(f\"\\nControl Model: \\n\")\n",
    "print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "# ROME edited models have become moon landing deniers?!\n",
    "print(f\"\\nROME Testing Model: \\n\")\n",
    "print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n",
    "\n",
    "print(f\"\\nROME Testing Model With Concept Erasure Hook: \\n\")\n",
    "with erasure_hook(testing_model, cav, layer_idx=16):\n",
    "    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddbdf6-a62f-4bd4-9cd7-d37128a883ea",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "This is not the final implementation I used in the game, but is still a handy demo using a popular vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a275fef-3523-4f43-bd02-1934af77a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6647571d-6591-4155-b4d6-db0716666b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs[\"output_hidden_states\"] = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    last_hidden = outputs.hidden_states[-1]\n",
    "    pooled = last_hidden.mean(dim=1)  # [batch_size, hidden_size]\n",
    "    return pooled[0].cpu().numpy()  # shape: (hidden_size,)\n",
    "\n",
    "print(get_embedding(\"test\").shape)  # Should be (2048,) or whatever your model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20854f09-960d-45c6-ab3d-9e14a08545a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()  \n",
    "collection = chroma_client.create_collection(name=\"case_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67b887a0-c3bb-4809-a02e-0761bffc35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"Python is a popular programming language for machine learning.\",\n",
    "    \"Shakespeare wrote many famous plays and poems.\",\n",
    "    \"The mitochondria is the powerhouse of the cell.\",\n",
    "    \"The butler did it!\",\n",
    "    \"Go is a deep strategic board game\",\n",
    "    \"The Stradivarius family are high end violin makers\"\n",
    "]\n",
    "\n",
    "# Generate embeddings and add to the collection\n",
    "for i, doc in enumerate(docs):\n",
    "    embedding = get_embedding(doc)\n",
    "    collection.add(documents=[doc], embeddings=[embedding.tolist()], ids=[f\"doc{i}\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "135af3d9-dcea-4509-acf9-8cbb2fccdc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top documents retrieved:\n",
      "- The butler did it!\n",
      "- The Eiffel Tower is located in Paris.\n",
      "<|system|>\n",
      "You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner. Consider the following trusted documents when responding: The butler did it! and, The Eiffel Tower is located in Paris. and,  \n",
      "<|user|>\n",
      "Who wrote Hamlet? \n",
      "<|assistant|>\n",
      "The play \"Hamlet\" by William Shakespeare is the author of the play.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who wrote Hamlet?\"\n",
    "query_embedding = get_embedding(query).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "# search vector db and incorporate results in system prompt\n",
    "rag_component = f\" Consider the following trusted documents when responding: \"\n",
    "print(\"Top documents retrieved:\")\n",
    "for doc in results[\"documents\"][0]:\n",
    "    print(\"-\", doc)\n",
    "    rag_component += doc + \" and, \"\n",
    "\n",
    "user_prompt = query\n",
    "system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n",
    "\n",
    "# For some reason the top result for a Hamlet query is not Shakespeare, its the Eiffel Tower? Weird.\n",
    "print(prompt_response(model, tokenizer, system_prompt + rag_component, user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4274b9-6f3a-4335-b5ee-abcae0dcb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"case_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c6ce0-ad54-4a82-911b-9a6c2297492e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
